{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loop and auxilary functions for NSD project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from dataset_pytorch import NSDDataset\n",
    "from net import Net\n",
    "from loss import ContrastiveLoss, TripletLoss\n",
    "from visualization import plot_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters and network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "n_iters = int(1e6)\n",
    "current_epoch = 0\n",
    "batch_size = 5\n",
    "learning_rate = 1e-4\n",
    "num_classes = 11\n",
    "\n",
    "# Network parameters\n",
    "training_type = 'clustering'\n",
    "channels = [72, 128, 128, 128, 2] # Channels to be used in each ResBlock\n",
    "reductions = ['mean', 'max', 'max', None] # Reduction to be used in ResBlock ('max', 'mean', None)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NSDDataset('saved_data.pickle') # WARNING - dataset takes a lot of RAM (around 6 GB), loading onto insufficient device could cause crashing\n",
    "\n",
    "model = Net(channels=channels, reductions=reductions, num_classes=num_classes, output_dimension=2, train_type=training_type).to(device)\n",
    "\n",
    "if training_type == 'clustering':\n",
    "    loss_function = torch.nn.TripletMarginLoss()\n",
    "elif training_type == 'classification':\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for results validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Function to calculate accuracy on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop - classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert training_type == 'classification'\n",
    "i = 0\n",
    "while current_epoch <= n_epochs:\n",
    "    # Load data\n",
    "    targets, inputs = dataset.get_batch(batch_size=batch_size)\n",
    "    i += 1\n",
    "    targets = targets.to(device)\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # Reset gradient after each iteration\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    net_output = model(inputs)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss = loss_function(net_output, targets)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print during epoch (sanity check)\n",
    "    with torch.no_grad():\n",
    "        print(f\"\\rEpoch: {current_epoch + 1} Iter: [{i} / {int(len(dataset) / batch_size) + 1}]| Loss: {round(loss.item(), 3)}\", end='')\n",
    "    \n",
    "    # Print after each epoch to get current results\n",
    "    if current_epoch != dataset.epoch:\n",
    "        current_epoch = dataset.epoch\n",
    "        torch.save(model.state_dict(), f'models/model_{current_epoch}.pth')\n",
    "        i = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop - clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0435736179351807\n",
      "0.999917209148407\n",
      "1.0000665187835693\n",
      "0.9997407793998718\n",
      "0.999994158744812\n",
      "1.0002386569976807\n",
      "1.0001848936080933\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\brejt\\OneDrive\\Plocha\\NUDZ\\train.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brejt/OneDrive/Plocha/NUDZ/train.ipynb#X20sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brejt/OneDrive/Plocha/NUDZ/train.ipynb#X20sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/brejt/OneDrive/Plocha/NUDZ/train.ipynb#X20sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mprint\u001b[39m(loss\u001b[39m.\u001b[39;49mitem())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brejt/OneDrive/Plocha/NUDZ/train.ipynb#X20sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m print_frequency \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brejt/OneDrive/Plocha/NUDZ/train.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     model\u001b[39m.\u001b[39meval()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert training_type == 'clustering'\n",
    "\n",
    "print_frequency = 200\n",
    "batch_size = 3\n",
    "\n",
    "\n",
    "for i in range(n_iters):\n",
    "    optimizer.zero_grad()\n",
    "    l_all = torch.zeros((batch_size)).to(device)\n",
    "    for b in range(batch_size):\n",
    "        a, p, n = dataset.get_triplet()\n",
    "        data_input = torch.stack([a, p, n]).to(device)\n",
    "        data_output = F.sigmoid(model(data_input))\n",
    "        af, pf, nf = torch.split(data_output, split_size_or_sections=1, dim=0)\n",
    "\n",
    "        loss = loss_function(af, pf, nf)\n",
    "        l_all[b] = loss\n",
    "\n",
    "        torch.nn.utils.clip_grad.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "    l_all = l_all.sum()\n",
    "    l_all.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(loss.item())\n",
    "\n",
    "    if i % print_frequency == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = torch.zeros((2, len(dataset))).to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
